{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "668f874c",
   "metadata": {},
   "source": [
    "Q1: What are the Probability Mass Function (PMF) and Probability Density Function (PDF)? Explain with\n",
    "an example.\n",
    "\n",
    "Ans:Both Probability Mass Function (PMF) and Probability Density Function (PDF) are used to describe the probability distribution of a random variable.\n",
    "\n",
    "PMF is used for discrete random variables, while PDF is used for continuous random variables.\n",
    "\n",
    "PMF describes the probability distribution of a discrete random variable by assigning a probability to each possible outcome. The PMF maps each value of the discrete random variable to the probability of that value occurring. Mathematically, PMF is defined as:\n",
    "\n",
    "P(X = x) = probability that X takes the value x\n",
    "\n",
    "where X is a discrete random variable and x is a possible value of X.\n",
    "\n",
    "For example, consider a random variable X that represents the number of heads obtained when a fair coin is flipped three times. The possible values of X are 0, 1, 2, or 3. The PMF for X is:\n",
    "\n",
    "P(X = 0) = 1/8\n",
    "P(X = 1) = 3/8\n",
    "P(X = 2) = 3/8\n",
    "P(X = 3) = 1/8\n",
    "\n",
    "PDF, on the other hand, describes the probability distribution of a continuous random variable by specifying the relative likelihood of each possible value that the variable can take on. PDF maps each value of the continuous random variable to the density of the probability at that value. Mathematically, PDF is defined as:\n",
    "\n",
    "f(x) = probability density function of a continuous random variable X\n",
    "\n",
    "For example, consider a random variable Y that represents the height of adult men in a certain population. The PDF for Y might be a normal distribution with mean 5'9\" (69 inches) and standard deviation of 2 inches. The PDF for Y can be written as:\n",
    "\n",
    "f(y) = 1/(sqrt(2*pi)sigma) * exp(-(y-mu)^2/(2sigma^2))\n",
    "\n",
    "where mu = 69 inches and sigma = 2 inches.\n",
    "\n",
    "While the value of the PDF at any given point represents the relative likelihood of that point occurring, the probability of a specific value occurring in a continuous distribution is zero, since there are an infinite number of possible values that the random variable can take on. Instead, we must consider the probability of the variable falling within a certain range, which can be calculated by integrating the PDF over that range.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a835a4",
   "metadata": {},
   "source": [
    "Q2: What is Cumulative Density Function (CDF)? Explain with an example. Why CDF is used?\n",
    "\n",
    "Ans:The cumulative distribution function (CDF) calculates the cumulative probability for a given x-value. Use the CDF to determine the likelihood that a random observation taken from the population will be less than or equal to a particular value. An example of a CMF:\n",
    "\n",
    "Let's say we're flipping a fair coin, and we're interested in the probability of getting heads. The random variable in this case is the outcome of the coin flip, which can either be heads or tails.\n",
    "\n",
    "The CMF of this random variable can be described as follows:\n",
    "\n",
    "If we flip the coin once, the probability of getting heads is 0.5, and the probability of getting tails is also 0.5. So the CMF at this point is:\n",
    "\n",
    "P(X <= 0) = 0.5 (where X is the random variable representing the outcome of the coin flip)\n",
    "\n",
    "If we flip the coin twice, there are four possible outcomes: HH, HT, TH, and TT. The probability of getting two heads is 0.25, the probability of getting two tails is 0.25, and the probability of getting one head and one tail is 0.5. The CMF at this point is:\n",
    "\n",
    "P(X <= 0) = 0.5 (same as before)\n",
    "P(X <= 1) = 0.75 (since there's a 0.5 probability of getting one head or one tail)\n",
    "P(X <= 2) = 1.0 (since there's a 0.25 probability of getting two heads or two tails)\n",
    "\n",
    "The CMF gives us the cumulative probabilities up to a certain point, so in this case, P(X <= k) gives us the probability of getting heads up to k coin flips.\n",
    "The Cumulative Distribution Function (CDF) is used to describe the probability distribution of a random variable. It is a function that gives the probability that a random variable X is less than or equal to a certain value x."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f79f4ee",
   "metadata": {},
   "source": [
    "Q3: What are some examples of situations where the normal distribution might be used as a model?\n",
    "Explain how the parameters of the normal distribution relate to the shape of the distribution.\n",
    "\n",
    "Ans:The normal distribution is a bell-shaped probability distribution that is commonly used as a model in many statistical applications. Here are some examples of situations where the normal distribution might be used as a model:\n",
    "\n",
    "Heights of people in a population\n",
    "Weights of objects produced in a manufacturing process\n",
    "Errors in measurement or observations\n",
    "IQ scores in a population\n",
    "Stock prices over time\n",
    "The parameters of the normal distribution are the mean (μ) and standard deviation (σ). The mean is the average value of the data, and the standard deviation is a measure of how spread out the data is. The larger the standard deviation, the more spread out the data is, and the flatter the distribution will be. Conversely, the smaller the standard deviation, the less spread out the data is, and the more peaked the distribution will be.\n",
    "\n",
    "The mean determines the location of the center of the distribution, while the standard deviation determines the shape of the distribution. In particular, the normal distribution is symmetric around the mean, meaning that the left and right tails of the distribution are equally balanced. Additionally, the standard deviation determines how quickly the tails of the distribution decay. A smaller standard deviation means that the tails decay more slowly, resulting in a wider distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3beceb",
   "metadata": {},
   "source": [
    "Q4: Explain the importance of Normal Distribution. Give a few real-life examples of Normal\n",
    "Distribution.\n",
    "Ans:The Normal Distribution is important in statistics and probability theory because it is a very common probability distribution that arises naturally in many real-life situations. It is used extensively in statistics for hypothesis testing, confidence intervals, and in regression analysis.\n",
    "\n",
    "The Normal Distribution is important because of the Central Limit Theorem (CLT), which states that if we have a large sample size from any population, the sampling distribution of the sample mean will be approximately normal, regardless of the shape of the population. This makes the normal distribution a very useful tool in statistical inference, as many statistical tests and procedures assume or rely on normality.\n",
    "Examples of Normal disribution are \n",
    "1-Heights and weights of individuals in a population.\n",
    "2-IQ scores in a population.\n",
    "3-Scores on standardized tests, such as the SAT or GRE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2b9776",
   "metadata": {},
   "source": [
    "Q5: What is Bernaulli Distribution? Give an Example. What is the difference between Bernoulli\n",
    "Distribution and Binomial Distribution?\n",
    "Ans:The Bernoulli Distribution is a probability distribution that describes the outcomes of a single binary experiment, where there are only two possible outcomes: success or failure. The Bernoulli distribution is named after the Swiss mathematician Jacob Bernoulli. The probability of success is denoted by p and the probability of failure is denoted by q, where q = 1 - p.\n",
    "\n",
    "An example of the Bernoulli Distribution is the outcome of a coin flip. If we consider \"heads\" to be a success and \"tails\" to be a failure, then the Bernoulli distribution can be used to model the probability of getting heads on a single coin flip. If the coin is fair, then the probability of getting heads is 0.5, so p = 0.5 and q = 1 - 0.5 = 0.5.\n",
    "\n",
    "The Binomial Distribution, on the other hand, is a probability distribution that describes the number of successes in a fixed number of independent Bernoulli trials. In other words, the Binomial Distribution is the sum of independent and identically distributed Bernoulli random variables. The Binomial Distribution is characterized by two parameters: n, the number of trials, and p, the probability of success in each trial.\n",
    "\n",
    "The difference between the Bernoulli Distribution and the Binomial Distribution is that the Bernoulli Distribution is used to model the outcome of a single binary experiment, while the Binomial Distribution is used to model the number of successes in a fixed number of independent Bernoulli trials. In other words, the Bernoulli Distribution is a special case of the Binomial Distribution, where n = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b9bf8e",
   "metadata": {},
   "source": [
    "Q6. Consider a dataset with a mean of 50 and a standard deviation of 10. If we assume that the dataset\n",
    "is normally distributed, what is the probability that a randomly selected observation will be greater\n",
    "than 60? Use the appropriate formula and show your calculations.\n",
    "Ans:To find the probability that a randomly selected observation will be greater than 60, we need to use the standard normal distribution and convert the given dataset with a mean of 50 and a standard deviation of 10 into a standard normal distribution with a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "The formula for standardizing a normal distribution is:\n",
    "\n",
    "z = (x - μ) / σ\n",
    "\n",
    "where x is the observation, μ is the mean, and σ is the standard deviation.\n",
    "\n",
    "Using this formula, we can standardize the value of 60 as follows:\n",
    "\n",
    "z = (60 - 50) / 10 = 1\n",
    "\n",
    "This means that 60 is one standard deviation above the mean.\n",
    "\n",
    "To find the probability of a value greater than 60, we can use a standard normal distribution table or a calculator and look up the probability associated with a z-score of 1. The probability of a value being greater than 60 is the area under the standard normal distribution curve to the right of z = 1.\n",
    "\n",
    "Using a standard normal distribution table, we find that the probability of a value being greater than 60 is approximately 0.1587.\n",
    "\n",
    "Therefore, the probability that a randomly selected observation will be greater than 60 is approximately 0.1587 or 15.87%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c789afa4",
   "metadata": {},
   "source": [
    "Q7: Explain uniform Distribution with an example.\n",
    "Ans:The Uniform Distribution is a probability distribution in which all possible outcomes are equally likely. It is a continuous probability distribution where the probability of any value between the minimum and maximum values is the same. The probability density function (PDF) of a Uniform Distribution is a horizontal line between the minimum and maximum values.\n",
    "\n",
    "An example of the Uniform Distribution is rolling a fair six-sided die. Each number on the die has an equal probability of 1/6 of being rolled. The minimum value is 1 and the maximum value is 6, and all values between 1 and 6 have an equal probability of occurring.\n",
    "\n",
    "Another example of the Uniform Distribution is selecting a random number between 0 and 1. The minimum value is 0 and the maximum value is 1, and all values between 0 and 1 have an equal probability of being selected.\n",
    "\n",
    "The Uniform Distribution is useful in probability theory and statistics because it provides a simple and intuitive model for situations where all outcomes are equally likely. It is also used in simulation and modeling, where a random number generator is used to generate uniformly distributed random numbers between a minimum and maximum value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c88ad9",
   "metadata": {},
   "source": [
    "Q8: What is the z score? State the importance of the z score.\n",
    "Ans:The z-score, also known as the standard score, is a measure of how many standard deviations an observation or data point is away from the mean of a dataset. It is calculated by subtracting the mean of the dataset from the data point and then dividing the difference by the standard deviation of the dataset.\n",
    "\n",
    "The formula for calculating the z-score is:\n",
    "\n",
    "z = (x - μ) / σ\n",
    "\n",
    "where x is the observation or data point, μ is the mean of the dataset, and σ is the standard deviation of the dataset.\n",
    "\n",
    "The z-score tells us how far away a data point is from the mean in terms of standard deviations. A positive z-score indicates that the data point is above the mean, while a negative z-score indicates that the data point is below the mean. A z-score of 0 indicates that the data point is equal to the mean.\n",
    "\n",
    "The importance of the z-score is that it provides a standardized way of comparing data points from different datasets that have different scales and units. By standardizing the data using the z-score, we can compare data points and determine how unusual or extreme they are relative to the mean of the dataset.\n",
    "\n",
    "The z-score is also useful in hypothesis testing and statistical inference. It can be used to calculate p-values and confidence intervals, which help us make statistical inferences about population parameters based on sample data.\n",
    "\n",
    "In summary, the z-score is an important statistical tool that helps us standardize and compare data points from different datasets, and it is widely used in statistical inference and hypothesis testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8ae5cf",
   "metadata": {},
   "source": [
    "Q9: What is Central Limit Theorem? State the significance of the Central Limit Theorem.\n",
    "Ans:The Central Limit Theorem (CLT) is a fundamental concept in probability theory and statistics. It states that if a sample is drawn from any population, regardless of the shape of the population distribution, as long as the sample size is large enough, the distribution of the sample means will be approximately normal.\n",
    "\n",
    "In other words, the Central Limit Theorem states that as the sample size increases, the distribution of the sample means will become more and more normal, even if the population distribution is not normal.\n",
    "\n",
    "The significance of the Central Limit Theorem is that it allows us to use normal distribution-based methods for inference and hypothesis testing, even when the population distribution is not known or not normal. This is because the distribution of the sample means will be approximately normal, regardless of the shape of the population distribution, as long as the sample size is large enough.\n",
    "\n",
    "The Central Limit Theorem is also important in practical applications of statistics, as it allows us to estimate population parameters such as the mean and standard deviation from sample data, and to make inferences about the population based on the sample data. It is widely used in fields such as economics, engineering, and social sciences, where sample data is commonly used to make inferences about the population."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cede3af9",
   "metadata": {},
   "source": [
    "Q10: State the assumptions of the Central Limit Theorem.\n",
    "Ans:The Central Limit Theorem (CLT) is a fundamental concept in probability theory and statistics. While it is a powerful tool for making inferences and predictions from sample data, it is important to understand the assumptions underlying the theorem. The assumptions of the Central Limit Theorem are:\n",
    "\n",
    "1-Independence: The observations in the sample are independent of each other. This means that the value of one observation does not affect the value of any other observation.\n",
    "\n",
    "2-Randomness: The sample is drawn randomly from the population. This means that every member of the population has an equal chance of being selected for the sample.\n",
    "\n",
    "3-Finite Variance: The population has a finite variance. This means that the variance of the population distribution is not infinite.\n",
    "\n",
    "4-Sample Size: The sample size is sufficiently large. While there is no hard and fast rule for how large the sample size must be, a commonly used rule of thumb is that the sample size should be at least 30.\n",
    "\n",
    "It is important to note that violating any of these assumptions can affect the validity of the Central Limit Theorem. In particular, violating the assumption of independence can lead to biased estimates and incorrect inferences. Therefore, it is important to carefully consider these assumptions when applying the Central Limit Theorem in practice.1-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987f1113",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9bce14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
